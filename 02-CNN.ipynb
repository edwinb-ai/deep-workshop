{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes Neuronales Convolucionales (CNN)\n",
    "\n",
    "Son un tipo de redes neuronales específicamente diseñadas y creadas para tratar con información que tiene topología o estructura de cuadrícula, como series de tiempo y sobre todo, imágenes.\n",
    "\n",
    "Estas redes neuronales se caracterizan por emplear la _convolución_ para aprender las características más básicas de las imágenes que se le suministran, con lo que pueden generalizar los elementos e identificar estructuras muy variadas a partir de estos bloques elementales.\n",
    "\n",
    "## Sobre la convolución\n",
    "\n",
    "Siguiendo el libro [Deep Learning](https://www.deeplearningbook.org/), la operación que emplean las CNN no es técnicamente la convolución sino una operación muy semejante llamanda [relación cruzada (cross-relation)](https://en.wikipedia.org/wiki/Cross-correlation), que es igual a la convolución en un caso particular.\n",
    "\n",
    "La razón de esto es porque computacionalmente es más eficiente programar y ejecutar la _relación cruzada_ que la convolución directamente, y esto es a lo que se le llama _convolución_ en _deep learning._\n",
    "\n",
    "## Caso de estudio: Clasificación de MNIST\n",
    "\n",
    "Para este documento se pretende clasificar el conjunto MNIST con una CNN básica, donde se proponen un par de estructuras elementales como las capas de convolución y de _pooling_. La idea es generar una CNN elemental que demuestre y explique la implementación en `keras`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers, models, datasets, utils\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hiperparámetros de la CNN\n",
    "\n",
    "Para comenzar se fijarán algunos hiperparámetros de la CNN, en esto caso se toma como ejemplo el [ejercicio de `keras` sobre CNN](https://keras.io/examples/mnist_cnn/) y se va a modificar para este documento.\n",
    "\n",
    "Los hiperparámetros se quedan igual, sobre todo el tamaño de las imágenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tamaño de lote para el optimizador\n",
    "batch_size = 128\n",
    "# Número de clases, ya se sabe por que se conoce el conjunto de datos\n",
    "# pero si no, se puede hacer de forma programática\n",
    "num_classes = 10\n",
    "# Número de épocas, dejar como está porque es suficiente\n",
    "epochs = 12\n",
    "\n",
    "# Tamaño de las imágenes, ancho y alto\n",
    "img_rows, img_cols = 28, 28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importar los datos\n",
    "\n",
    "Se extraen los datos desde la API de `keras` la cual ya separa el conjunto de datos en entrenamiento y prueba. En total el conjunto de datos consta de 70000 imágenes y aquí se separan en 60000 de entrenamiento y 10000 de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28, 1)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Para verificar el tamaño de las imágenes y el número de imágenes\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f6b6f09ae50>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Se puede visualizar una de estas imágenes para conocer el conjunto de datos\n",
    "plt.imshow(x_train[0, :, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesamiento de datos\n",
    "\n",
    "Ahora se deben ajustar los datos de entrada para que sean consistentes con las capas de la CNN en `keras`. En este caso se va a agregar una dimensión adicional hasta el final del _tensor_ de datos para afirmar que existe solamente un canal de colores.\n",
    "\n",
    "El tamaño de las imágenes es ahora **(número de ejemplos, ancho, alto, número de canales)** en ese orden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reajustar las imágenes para asegurar que solamente es un canal de color\n",
    "x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "# Tamaño de la entrada, en forma de tupla siempre\n",
    "input_shape = (img_rows, img_cols, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un paso adicional para asegurar que todo es consistente es convertir los datos a `float32` para mejor manejo con el CPU/GPU (recordar que las GPU solamente pueden trabajar con flotantes de 32 bits).\n",
    "\n",
    "Adicionalmente, se normalizan las entradas para dejar en flotantes los valores de cada pixel, dividiendo por el total que pueden adquirir como valor. Esto para seguir con el manejo de la CNN y consistencia a lo largo del entrenamiento y evaluación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir a float32 para mejor manejo\n",
    "x_train = x_train.astype(\"float32\")\n",
    "x_test = x_test.astype(\"float32\")\n",
    "# Normalizar los valores de las imágenes\n",
    "x_train /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del conjunto de entrenamiento: (60000, 28, 28, 1)\n",
      "60000 Muestras de entrenamiento\n",
      "10000 Muestras de prueba\n"
     ]
    }
   ],
   "source": [
    "print(\"Tamaño del conjunto de entrenamiento:\", x_train.shape)\n",
    "print(x_train.shape[0], \"Muestras de entrenamiento\")\n",
    "print(x_test.shape[0], \"Muestras de prueba\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con esto nos aseguramos de todos los pasos que hemos hecho hasta ahora. Todo parece estar en orden, teniendo 60000 imágenes de entrenamiento y 10000 de prueba.\n",
    "\n",
    "El último paso de preprocesamiento es hacer la codificación _one-hot_ para las etiquetas, que se puede realizar fácilmente en `keras`, o alternativamente en `scikit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Hot encoding para las etiquetas\n",
    "y_train = utils.to_categorical(y_train, num_classes)\n",
    "y_test = utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Está representación dice que esta entrada en particular pertenece al\n",
    "# dígito 6 en el conjunto de datos\n",
    "y_train[0, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arquitectura de la CNN\n",
    "\n",
    "Siguiendo el ejemplo de `keras` se toma la siguiente arquitectura:\n",
    "\n",
    "- La **entrada** consta de una capa convolucional de 32 unidades. El _kernel_ de convolución es de $3 \\times 3$ y tiene una función de activación ReLU. No tiene **relleno** en los bordes, por lo que al final de esta capa el tamaño de las imágenes se verá reducido.\n",
    "\n",
    "- La **primera capa oculta** consta de una capa convolucional de 64 unidades, con un _kernel_ de $3 \\times 3$ y función de activación ReLU. Después de esto se aplica una capa de agrupación o _pooling_ de tipo máximo, lo que seleccionará el máximo valor entre los 2 vecinos más cercanos, en este caso en particular.\n",
    "\n",
    "- La **segunda capa oculta** consta de un _aplanamiento_ espacial de las imágenes, lo que las hará vectores unidimensionales, y una red neuronal totalmente conectada de 128 unidades con función de activación ReLU.\n",
    "\n",
    "- La **capa de salida** consta del número total de clases y la función de activación _softmax_ para clasificación.\n",
    "\n",
    "Este diseño consta entonces de dos partes, la primera parte para aprender **características** mediante las dos capas convolucionales, y la última parte con dos capas para la **clasificación** efectiva del conjunto de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "arquitectura = [\n",
    "    layers.Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation=\"relu\",\n",
    "                 input_shape=input_shape),\n",
    "    layers.Conv2D(64, (3, 3), activation=\"relu\"),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation=\"relu\"),\n",
    "    layers.Dense(num_classes, activation=\"softmax\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se construye el modelo con este diseño\n",
    "model = models.Sequential(arquitectura)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimización y entrenamiento\n",
    "\n",
    "Para optimizar y medir la eficacia de este modelo se optó por el optimizador [NADAM](http://cs229.stanford.edu/proj2015/054_report.pdf) que facilita el aprendizaje en arquitecturas grandes al tener un ratio de aprendizaje autoajustable, y la _precisión_ como medida de clasificación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "              optimizer=\"nadam\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora se entrena el modelo, separando un 20% del conjunto de entrenamiento como conjunto de validación, para dejar el conjunto de prueba como evaluación de la generalización del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/12\n",
      "48000/48000 [==============================] - 53s 1ms/step - loss: 0.1617 - acc: 0.9508 - val_loss: 0.0559 - val_acc: 0.9846\n",
      "Epoch 2/12\n",
      "48000/48000 [==============================] - 53s 1ms/step - loss: 0.0417 - acc: 0.9873 - val_loss: 0.0485 - val_acc: 0.9850\n",
      "Epoch 3/12\n",
      "48000/48000 [==============================] - 53s 1ms/step - loss: 0.0244 - acc: 0.9923 - val_loss: 0.0452 - val_acc: 0.9871\n",
      "Epoch 4/12\n",
      "48000/48000 [==============================] - 53s 1ms/step - loss: 0.0146 - acc: 0.9953 - val_loss: 0.0459 - val_acc: 0.9877\n",
      "Epoch 5/12\n",
      "48000/48000 [==============================] - 53s 1ms/step - loss: 0.0105 - acc: 0.9964 - val_loss: 0.0529 - val_acc: 0.9862\n",
      "Epoch 6/12\n",
      "48000/48000 [==============================] - 53s 1ms/step - loss: 0.0102 - acc: 0.9966 - val_loss: 0.0510 - val_acc: 0.9888\n",
      "Epoch 7/12\n",
      "48000/48000 [==============================] - 53s 1ms/step - loss: 0.0074 - acc: 0.9975 - val_loss: 0.0569 - val_acc: 0.9875\n",
      "Epoch 8/12\n",
      "48000/48000 [==============================] - 54s 1ms/step - loss: 0.0089 - acc: 0.9968 - val_loss: 0.0635 - val_acc: 0.9864\n",
      "Epoch 9/12\n",
      "48000/48000 [==============================] - 54s 1ms/step - loss: 0.0086 - acc: 0.9971 - val_loss: 0.0538 - val_acc: 0.9892\n",
      "Epoch 10/12\n",
      "48000/48000 [==============================] - 53s 1ms/step - loss: 0.0056 - acc: 0.9980 - val_loss: 0.0643 - val_acc: 0.9869\n",
      "Epoch 11/12\n",
      "48000/48000 [==============================] - 54s 1ms/step - loss: 0.0051 - acc: 0.9983 - val_loss: 0.0655 - val_acc: 0.9874\n",
      "Epoch 12/12\n",
      "48000/48000 [==============================] - 54s 1ms/step - loss: 0.0062 - acc: 0.9980 - val_loss: 0.0573 - val_acc: 0.9897\n",
      "CPU times: user 28min 44s, sys: 18.5 s, total: 29min 3s\n",
      "Wall time: 10min 40s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6b6e707450>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si se observa el valor de `acc`, la precisión en el conjunto de entrenamiento, y `val_acc` la precisión en el conjunto de validación, existe una diferencia notable, con 99.6% y 98.7% en promedio, respectivamente.\n",
    "\n",
    "Esto implica que el modelo está **sobreajustado**, y aunque pareciera que son algunos decimales, en realidad está _bastante_ **sobreajustado**. Para comprobar este resultado, se evaluará el modelo entrenado en el conjunto de prueba que se reservó desde el principio, recordando que este conjunto nunca fue visto por el modelo; con esto se podrá comprobar la verdadera generalización del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pérdida general del conjunto de prueba: 0.04712263136188756\n",
      "Precisión del conjunto de prueba: 0.9899\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Pérdida general del conjunto de prueba:\", score[0])\n",
    "print(\"Precisión del conjunto de prueba:\", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como era de esperarse, la _precisión_ en el conjunto de prueba se parece más al valor encontrado por la precisión del conjunto de validación, esto es el valor de `val_acc` en el modelo antes mostrado.\n",
    "\n",
    "Esto demuestra que el modelo está efectivamente **sobreajustado**, y no por un margen pequeño (aunque lo parezca).\n",
    "\n",
    "La pregunta aquí es: ¿qué se puede hacer para mejorarlo? ¿Esto se debe al diseño de la CNN? ¿Se debe a la elección de hiperparámetros?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicios\n",
    "\n",
    "- La arquitectura de esta CNN no es única, ¿será posible mejorar el valor de precisión? Modificar el número de unidades en las capas excepto la salida para comprobar si esto es posible.\n",
    "\n",
    "- El sobreajuste del modelo se puede deber a varias razones, ¿cuáles son las más probables?\n",
    "\n",
    "- Cada capa convolucional, al no tener relleno, reduce el tamaño de cada imágen. Calcular estos tamaños y el tamaño final del vector de información para saber el efecto de tener relleno o no. ¿Qué pasaría si se le pone relleno? Experimentar con las opciones de relleno en `keras`.\n",
    "\n",
    "- La capa de _pooling_ no es única, modificar usando las distintas implementaciones de `keras`. ¿Esto afecta positivamente al resultado del modelo?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
