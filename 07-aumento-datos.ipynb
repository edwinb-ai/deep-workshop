{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aumento de datos\n",
    "\n",
    "No siempre se tienen muchos ejemplos y datos para trabajar con los modelos propuestos. Esta es una de las principales desventajas de utilizar modelos con demasiados parámetros por aprender, como es el caso de las redes neuronales y en especial las convolucionales (CNN).\n",
    "\n",
    "Sin embargo, una metodología que ha ganado muchos seguidores es el _aumento de datos._ En particular, esta técnica fue creada para imágenes pero posteriormente adaptada para otro tipo de datos, como secuencias y vocabularios. La idea es simple: modificar de alguna forma los ejemplos que se tienen para _hacer creer_ al modelo que son ejemplos _nuevos._\n",
    "\n",
    "Un caso de esto es agregar **ruido blanco** o gaussiano a las imágenes. Esto se ve como estática en las imágenes, lo que causa que el modelo piense que son imágenes diferentes, imágenes con ruido. Esto crea un efecto de tener más datos de los que en realidad se tienen.\n",
    "\n",
    "Esta no es la única técnica, también existe la **rotación,** **traslación,** **reflexión sobre un eje,** entre muchas otras. En este documento se van a presentar algunas de las más utilizadas y su efecto en la clasificación de un conjunto de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import fashion_mnist\n",
    "from keras.layers import Dense, BatchNormalization, Activation, Dropout\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, GaussianNoise\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras import regularizers\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fashion-MNIST\n",
    "\n",
    "El conjunto de datos empleado es el [Fashion-MNIST](https://github.com/zalandoresearch/fashion-mnist) que tiene 10 clases, cada una pertenece a un tipo de prenda o de calzado. La idea de este conjunto de datos (en realidad son imágenes) es que sirva como un reemplazo para el famoso conjunto de datos de dígitos escritos a mano del MNIST.\n",
    "\n",
    "Las imágenes tienen un tamaño de $28 \\times 28$ y están en tono de grises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tamaño de lote para el optimizador\n",
    "batch_size = 128\n",
    "# Número de clases, ya se sabe por que se conoce el conjunto de datos\n",
    "# pero si no, se puede hacer de forma programática\n",
    "num_classes = 10\n",
    "# Número de épocas, dejar como está porque es suficiente\n",
    "epocas = 100\n",
    "# Parámetro de regularización L2 (Ridge)\n",
    "regular_param_l2 = 1e-4\n",
    "# Parámetro de regularización L1 (Lasso)\n",
    "regular_param_l1 = 5e-5\n",
    "\n",
    "# Tamaño de las imágenes, ancho y alto\n",
    "img_rows, img_cols = 28, 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f036585c290>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Se puede visualizar una de estas imágenes para conocer el conjunto de datos\n",
    "plt.imshow(x_train[56, :, :], cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reajustar las imágenes para asegurar que solamente es un canal de color\n",
    "x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "# Tamaño de la entrada, en forma de tupla siempre\n",
    "input_shape = (img_rows, img_cols, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir a float32 para mejor manejo\n",
    "x_train = x_train.astype(\"float32\")\n",
    "x_test = x_test.astype(\"float32\")\n",
    "# Normalizar los valores de las imágenes\n",
    "x_train /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del conjunto de entrenamiento: (60000, 28, 28, 1)\n",
      "60000 Muestras de entrenamiento\n",
      "10000 Muestras de prueba\n"
     ]
    }
   ],
   "source": [
    "print(\"Tamaño del conjunto de entrenamiento:\", x_train.shape)\n",
    "print(x_train.shape[0], \"Muestras de entrenamiento\")\n",
    "print(x_test.shape[0], \"Muestras de prueba\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codificación One Hot para las etiquetas\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arquitecturas de las CNN\n",
    "\n",
    "Todas las arquitecturas tienen la misma forma en común, que proviene del [ejemplo de `keras`](https://keras.io/examples/cifar10_cnn/) para la clasificación del conjunto de datos CIFAR-10, solamente que en este caso se emplea otro conjunto de datos como ya se ha explicado anteriormente.\n",
    "\n",
    "En esta arquitectura se empleará la regularización _ElasticNet_ para los pesos dentro de la CNN. También se va a emplear la regularización _dropout_ con el 25% de unidades de características y un 50% de unidades en la capa totalmente conectada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Con regularizador L1 y L2\n",
    "arquitectura_l1_l2 = [\n",
    "    # Capa de entrada, primera capa de características\n",
    "    Conv2D(32, kernel_size=(3, 3),\n",
    "           padding=\"same\",\n",
    "           input_shape=x_train.shape[1:],\n",
    "          kernel_regularizer=regularizers.l1_l2(l1=regular_param_l1, l2=regular_param_l2)),\n",
    "    Activation(\"relu\"),\n",
    "    Conv2D(32, (3, 3), kernel_regularizer=regularizers.l1_l2(l1=regular_param_l1, l2=regular_param_l2)),\n",
    "    Activation(\"relu\"),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Dropout(0.25),\n",
    "    # Segunda capa de características\n",
    "    Conv2D(64, kernel_size=(3, 3), padding=\"same\",\n",
    "          kernel_regularizer=regularizers.l1_l2(l1=regular_param_l1, l2=regular_param_l2)),\n",
    "    Activation(\"relu\"),\n",
    "    Conv2D(64, (3, 3), kernel_regularizer=regularizers.l1_l2(l1=regular_param_l1, l2=regular_param_l2)),\n",
    "    Activation(\"relu\"),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Dropout(0.25),\n",
    "    # Capa totalmente conectada\n",
    "    Flatten(),\n",
    "    Dense(512, kernel_regularizer=regularizers.l1_l2(l1=regular_param_l1, l2=regular_param_l2)),\n",
    "    Dropout(0.5),\n",
    "    Activation(\"relu\"),\n",
    "    # Capa de salida\n",
    "    Dense(num_classes, activation=\"softmax\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_l1_l2 = Sequential(arquitectura_l1_l2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimización y entrenamiento\n",
    "\n",
    "Este modelo será entrenado con descenso de gradiente estocástico con ratio de aprendizaje 0.001 y momento clásico de 0.9.\n",
    "\n",
    "El entrenamiento será validado con el 20% de ejemplos del conjunto de entrenamiento, y después será evaluado con el conjunto de prueba que se separó al principio, en el preprocesamiento de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_l1_l2.compile(loss=\"categorical_crossentropy\",\n",
    "                 optimizer=SGD(lr=0.001, momentum=0.9),\n",
    "                 metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesamiento de datos junto con aumento\n",
    "\n",
    "En esta parte se implementa la utilidad de `keras` que se llama `ImageDataGenerator`. Esta utilidad toma un conjunto de datos y los procesa, luego va generando de forma _retardada_ los conjuntos de datos según el modelo los va solicitando.\n",
    "\n",
    "En este caso se van a utilizar las siguientes transformaciones:\n",
    "\n",
    "- La **traslación** con un valor de 0.1 hacia _verticalmente_ y _horizontalmente._\n",
    "\n",
    "- Se va a utilizar el **acercamiento**, con un valor de aumento de 20%.\n",
    "\n",
    "- **Relleno** en las orillas de la imagen empleando el método de _vecinos más cercanos._\n",
    "\n",
    "- **Reflexión** sobre los ejes vertical y horizontales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # si es positivo, la media del conjunto de datos es 0\n",
    "        samplewise_center=False,  # si es positivo, la medida de cada ejemplo es 0\n",
    "        featurewise_std_normalization=False,  # divide el conjunto de datos por la desviación estándar\n",
    "        samplewise_std_normalization=False,  # divide cada ejemplo por su propia desviación estándar\n",
    "        zca_whitening=False,  # aplicar ruido tipo Zero-phase Component Analysis\n",
    "        zca_epsilon=1e-06,  # valor del ruido tipo ZCA\n",
    "        rotation_range=0,  # se rota aleatoriamente la imagen, el valor es entero de 0 a 180\n",
    "        # traslación horizontal aleatoria, la fracción representa cuánto\n",
    "        width_shift_range=0.1,\n",
    "        # traslación vertical aleatoria, la fracción representa cuánto\n",
    "        height_shift_range=0.1,\n",
    "        shear_range=0.0,  # valor para distorsión\n",
    "        zoom_range=0.2,  # valor para acercamiento\n",
    "        channel_shift_range=0.,  # cambio de canales, de forma aleatoria\n",
    "        # modo para rellenado de pixeles fuera de la imagen\n",
    "        fill_mode=\"nearest\",\n",
    "        cval=0.,  # en caso de que el rellenado sea \"constant\"\n",
    "        horizontal_flip=True,  # reflexión horizontal aleatoria de las imágenes\n",
    "        vertical_flip=True,  # reflexión vertical aleatoria de las imágenes\n",
    "        # factor de reescalamiento (se aplica primero antes que cualquier transformación)\n",
    "        rescale=None,\n",
    "        # algún tipo de función definida por el usuario que haga transformaciones\n",
    "        # i.e. filtro de Borel, transformada de Fourier, ruido rosa, etc.\n",
    "        preprocessing_function=None,\n",
    "        # especificar el tipo de datos que se están ingresando\n",
    "        data_format=None,\n",
    "        # fracción de imágenes para separar en entrenamieto y validación\n",
    "        validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con esto se lleva a cabo el entrenamiento, sin embargo se debe hacer de forma diferente. La cuestión es que `keras` crea las imágenes _en el momento_ por lo que el entrenamiento también debe ser de esta forma. Para este tipo de procesamiento se debe de llamar las funciones de entrenamiento de otra forma, como se verá a continuación.\n",
    "\n",
    "También es importante notar que como son _flujos de datos,_ el modelo no conoce el tamaño del flujo antes de procesarlo, por lo que hay que definirlo mediante las opciones `steps_per_epoch` y `validation_steps`, como se muestra a continuación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "468/468 [==============================] - 111s 237ms/step - loss: 3.4181 - acc: 0.2074 - val_loss: 2.8694 - val_acc: 0.4313\n",
      "Epoch 2/100\n",
      "468/468 [==============================] - 128s 273ms/step - loss: 2.7973 - acc: 0.4226 - val_loss: 2.5007 - val_acc: 0.5653\n",
      "Epoch 3/100\n",
      "468/468 [==============================] - 141s 301ms/step - loss: 2.5460 - acc: 0.5107 - val_loss: 2.3463 - val_acc: 0.6160\n",
      "Epoch 4/100\n",
      "468/468 [==============================] - 148s 316ms/step - loss: 2.4057 - acc: 0.5666 - val_loss: 2.2348 - val_acc: 0.6440\n",
      "Epoch 5/100\n",
      "468/468 [==============================] - 126s 268ms/step - loss: 2.3009 - acc: 0.6041 - val_loss: 2.1437 - val_acc: 0.6696\n",
      "Epoch 6/100\n",
      "468/468 [==============================] - 101s 216ms/step - loss: 2.2158 - acc: 0.6342 - val_loss: 2.0715 - val_acc: 0.6927\n",
      "Epoch 7/100\n",
      "468/468 [==============================] - 104s 222ms/step - loss: 2.1571 - acc: 0.6514 - val_loss: 2.0518 - val_acc: 0.6907\n",
      "Epoch 8/100\n",
      "468/468 [==============================] - 103s 220ms/step - loss: 2.1106 - acc: 0.6644 - val_loss: 1.9941 - val_acc: 0.7120\n",
      "Epoch 9/100\n",
      "468/468 [==============================] - 102s 218ms/step - loss: 2.0739 - acc: 0.6746 - val_loss: 1.9704 - val_acc: 0.7090\n",
      "Epoch 10/100\n",
      "468/468 [==============================] - 103s 220ms/step - loss: 2.0435 - acc: 0.6828 - val_loss: 1.9342 - val_acc: 0.7209\n",
      "Epoch 11/100\n",
      "468/468 [==============================] - 102s 218ms/step - loss: 2.0087 - acc: 0.6907 - val_loss: 1.9038 - val_acc: 0.7276\n",
      "Epoch 12/100\n",
      "468/468 [==============================] - 108s 232ms/step - loss: 1.9843 - acc: 0.6961 - val_loss: 1.8782 - val_acc: 0.7324\n",
      "Epoch 13/100\n",
      "468/468 [==============================] - 106s 227ms/step - loss: 1.9550 - acc: 0.7024 - val_loss: 1.8569 - val_acc: 0.7373\n",
      "Epoch 14/100\n",
      "468/468 [==============================] - 106s 227ms/step - loss: 1.9347 - acc: 0.7039 - val_loss: 1.8383 - val_acc: 0.7349\n",
      "Epoch 15/100\n",
      "468/468 [==============================] - 105s 224ms/step - loss: 1.9165 - acc: 0.7063 - val_loss: 1.8121 - val_acc: 0.7445\n",
      "Epoch 16/100\n",
      "468/468 [==============================] - 107s 228ms/step - loss: 1.8867 - acc: 0.7143 - val_loss: 1.7909 - val_acc: 0.7465\n",
      "Epoch 17/100\n",
      "468/468 [==============================] - 106s 227ms/step - loss: 1.8674 - acc: 0.7178 - val_loss: 1.7755 - val_acc: 0.7481\n",
      "Epoch 18/100\n",
      "468/468 [==============================] - 107s 228ms/step - loss: 1.8474 - acc: 0.7188 - val_loss: 1.7539 - val_acc: 0.7484\n",
      "Epoch 19/100\n",
      "468/468 [==============================] - 109s 233ms/step - loss: 1.8294 - acc: 0.7222 - val_loss: 1.7311 - val_acc: 0.7592\n",
      "Epoch 20/100\n",
      "468/468 [==============================] - 110s 235ms/step - loss: 1.8070 - acc: 0.7269 - val_loss: 1.7169 - val_acc: 0.7562\n",
      "Epoch 21/100\n",
      "468/468 [==============================] - 107s 229ms/step - loss: 1.7862 - acc: 0.7293 - val_loss: 1.6921 - val_acc: 0.7617\n",
      "Epoch 22/100\n",
      "468/468 [==============================] - 106s 226ms/step - loss: 1.7713 - acc: 0.7312 - val_loss: 1.6737 - val_acc: 0.7680\n",
      "Epoch 23/100\n",
      "468/468 [==============================] - 112s 238ms/step - loss: 1.7517 - acc: 0.7364 - val_loss: 1.6514 - val_acc: 0.7709\n",
      "Epoch 24/100\n",
      "468/468 [==============================] - 106s 226ms/step - loss: 1.7350 - acc: 0.7383 - val_loss: 1.6475 - val_acc: 0.7666\n",
      "Epoch 25/100\n",
      "468/468 [==============================] - 111s 238ms/step - loss: 1.7210 - acc: 0.7388 - val_loss: 1.6301 - val_acc: 0.7707\n",
      "Epoch 26/100\n",
      "468/468 [==============================] - 108s 230ms/step - loss: 1.6939 - acc: 0.7422 - val_loss: 1.6075 - val_acc: 0.7739\n",
      "Epoch 27/100\n",
      "468/468 [==============================] - 107s 229ms/step - loss: 1.6775 - acc: 0.7473 - val_loss: 1.5955 - val_acc: 0.7765\n",
      "Epoch 28/100\n",
      "468/468 [==============================] - 111s 238ms/step - loss: 1.6665 - acc: 0.7480 - val_loss: 1.5713 - val_acc: 0.7835\n",
      "Epoch 29/100\n",
      "468/468 [==============================] - 115s 246ms/step - loss: 1.6494 - acc: 0.7502 - val_loss: 1.5583 - val_acc: 0.7824\n",
      "Epoch 30/100\n",
      "468/468 [==============================] - 105s 223ms/step - loss: 1.6335 - acc: 0.7524 - val_loss: 1.5401 - val_acc: 0.7852\n",
      "Epoch 31/100\n",
      "468/468 [==============================] - 115s 245ms/step - loss: 1.6191 - acc: 0.7527 - val_loss: 1.5235 - val_acc: 0.7889\n",
      "Epoch 32/100\n",
      "468/468 [==============================] - 157s 334ms/step - loss: 1.5970 - acc: 0.7598 - val_loss: 1.5027 - val_acc: 0.7913\n",
      "Epoch 33/100\n",
      "468/468 [==============================] - 159s 339ms/step - loss: 1.5848 - acc: 0.7596 - val_loss: 1.4971 - val_acc: 0.7914\n",
      "Epoch 34/100\n",
      "468/468 [==============================] - 157s 336ms/step - loss: 1.5691 - acc: 0.7609 - val_loss: 1.4705 - val_acc: 0.7968\n",
      "Epoch 35/100\n",
      "468/468 [==============================] - 162s 347ms/step - loss: 1.5493 - acc: 0.7650 - val_loss: 1.4581 - val_acc: 0.7984\n",
      "Epoch 36/100\n",
      "468/468 [==============================] - 158s 337ms/step - loss: 1.5376 - acc: 0.7675 - val_loss: 1.4468 - val_acc: 0.7999\n",
      "Epoch 37/100\n",
      "468/468 [==============================] - 155s 332ms/step - loss: 1.5260 - acc: 0.7679 - val_loss: 1.4391 - val_acc: 0.7995\n",
      "Epoch 38/100\n",
      "468/468 [==============================] - 126s 269ms/step - loss: 1.5115 - acc: 0.7679 - val_loss: 1.4271 - val_acc: 0.7980\n",
      "Epoch 39/100\n",
      "468/468 [==============================] - 111s 237ms/step - loss: 1.4923 - acc: 0.7727 - val_loss: 1.3992 - val_acc: 0.8070\n",
      "Epoch 40/100\n",
      "468/468 [==============================] - 104s 223ms/step - loss: 1.4801 - acc: 0.7739 - val_loss: 1.3868 - val_acc: 0.8076\n",
      "Epoch 41/100\n",
      "468/468 [==============================] - 104s 222ms/step - loss: 1.4679 - acc: 0.7745 - val_loss: 1.3741 - val_acc: 0.8109\n",
      "Epoch 42/100\n",
      "468/468 [==============================] - 103s 221ms/step - loss: 1.4561 - acc: 0.7765 - val_loss: 1.3632 - val_acc: 0.8088\n",
      "Epoch 43/100\n",
      "468/468 [==============================] - 114s 243ms/step - loss: 1.4379 - acc: 0.7808 - val_loss: 1.3701 - val_acc: 0.8007\n",
      "Epoch 44/100\n",
      "468/468 [==============================] - 109s 232ms/step - loss: 1.4254 - acc: 0.7808 - val_loss: 1.3344 - val_acc: 0.8150\n",
      "Epoch 45/100\n",
      "468/468 [==============================] - 111s 238ms/step - loss: 1.4135 - acc: 0.7808 - val_loss: 1.3266 - val_acc: 0.8122\n",
      "Epoch 46/100\n",
      "468/468 [==============================] - 110s 235ms/step - loss: 1.3995 - acc: 0.7830 - val_loss: 1.3049 - val_acc: 0.8175\n",
      "Epoch 47/100\n",
      "468/468 [==============================] - 108s 232ms/step - loss: 1.3900 - acc: 0.7832 - val_loss: 1.2952 - val_acc: 0.8179\n",
      "Epoch 48/100\n",
      "468/468 [==============================] - 110s 236ms/step - loss: 1.3701 - acc: 0.7883 - val_loss: 1.2822 - val_acc: 0.8215\n",
      "Epoch 49/100\n",
      "468/468 [==============================] - 104s 223ms/step - loss: 1.3599 - acc: 0.7887 - val_loss: 1.2782 - val_acc: 0.8138\n",
      "Epoch 50/100\n",
      "468/468 [==============================] - 111s 237ms/step - loss: 1.3516 - acc: 0.7882 - val_loss: 1.2567 - val_acc: 0.8231\n",
      "Epoch 51/100\n",
      "468/468 [==============================] - 107s 229ms/step - loss: 1.3353 - acc: 0.7907 - val_loss: 1.2485 - val_acc: 0.8215\n",
      "Epoch 52/100\n",
      "468/468 [==============================] - 105s 224ms/step - loss: 1.3255 - acc: 0.7932 - val_loss: 1.2354 - val_acc: 0.8252\n",
      "Epoch 53/100\n",
      "468/468 [==============================] - 103s 219ms/step - loss: 1.3155 - acc: 0.7918 - val_loss: 1.2281 - val_acc: 0.8236\n",
      "Epoch 54/100\n",
      "468/468 [==============================] - 108s 232ms/step - loss: 1.3009 - acc: 0.7953 - val_loss: 1.2136 - val_acc: 0.8252\n",
      "Epoch 55/100\n",
      "468/468 [==============================] - 103s 220ms/step - loss: 1.2858 - acc: 0.7966 - val_loss: 1.2122 - val_acc: 0.8219\n",
      "Epoch 56/100\n",
      "468/468 [==============================] - 107s 228ms/step - loss: 1.2791 - acc: 0.7958 - val_loss: 1.1890 - val_acc: 0.8298\n",
      "Epoch 57/100\n",
      "468/468 [==============================] - 103s 220ms/step - loss: 1.2711 - acc: 0.7964 - val_loss: 1.1928 - val_acc: 0.8275\n",
      "Epoch 58/100\n",
      "468/468 [==============================] - 101s 217ms/step - loss: 1.2568 - acc: 0.7985 - val_loss: 1.1840 - val_acc: 0.8236\n",
      "Epoch 59/100\n",
      "468/468 [==============================] - 102s 218ms/step - loss: 1.2425 - acc: 0.8007 - val_loss: 1.1584 - val_acc: 0.8315\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "468/468 [==============================] - 103s 219ms/step - loss: 1.2344 - acc: 0.8012 - val_loss: 1.1487 - val_acc: 0.8314\n",
      "Epoch 61/100\n",
      "468/468 [==============================] - 102s 218ms/step - loss: 1.2189 - acc: 0.8031 - val_loss: 1.1398 - val_acc: 0.8329\n",
      "Epoch 62/100\n",
      "468/468 [==============================] - 101s 215ms/step - loss: 1.2122 - acc: 0.8029 - val_loss: 1.1262 - val_acc: 0.8337\n",
      "Epoch 63/100\n",
      "468/468 [==============================] - 105s 225ms/step - loss: 1.1984 - acc: 0.8044 - val_loss: 1.1127 - val_acc: 0.8376\n",
      "Epoch 64/100\n",
      "468/468 [==============================] - 101s 216ms/step - loss: 1.1921 - acc: 0.8029 - val_loss: 1.1059 - val_acc: 0.8354\n",
      "Epoch 65/100\n",
      "468/468 [==============================] - 101s 215ms/step - loss: 1.1850 - acc: 0.8033 - val_loss: 1.0989 - val_acc: 0.8342\n",
      "Epoch 66/100\n",
      "468/468 [==============================] - 103s 220ms/step - loss: 1.1673 - acc: 0.8084 - val_loss: 1.0940 - val_acc: 0.8307\n",
      "Epoch 67/100\n",
      "468/468 [==============================] - 102s 219ms/step - loss: 1.1614 - acc: 0.8056 - val_loss: 1.0833 - val_acc: 0.8349\n",
      "Epoch 68/100\n",
      "468/468 [==============================] - 103s 221ms/step - loss: 1.1471 - acc: 0.8104 - val_loss: 1.0691 - val_acc: 0.8376\n",
      "Epoch 69/100\n",
      "468/468 [==============================] - 103s 220ms/step - loss: 1.1418 - acc: 0.8073 - val_loss: 1.0605 - val_acc: 0.8414\n",
      "Epoch 70/100\n",
      "468/468 [==============================] - 102s 217ms/step - loss: 1.1320 - acc: 0.8095 - val_loss: 1.0531 - val_acc: 0.8381\n",
      "Epoch 71/100\n",
      "468/468 [==============================] - 100s 213ms/step - loss: 1.1235 - acc: 0.8126 - val_loss: 1.0348 - val_acc: 0.8405\n",
      "Epoch 72/100\n",
      "468/468 [==============================] - 104s 222ms/step - loss: 1.1067 - acc: 0.8137 - val_loss: 1.0276 - val_acc: 0.8415\n",
      "Epoch 73/100\n",
      "468/468 [==============================] - 102s 218ms/step - loss: 1.1092 - acc: 0.8120 - val_loss: 1.0249 - val_acc: 0.8411\n",
      "Epoch 74/100\n",
      "468/468 [==============================] - 107s 229ms/step - loss: 1.0933 - acc: 0.8124 - val_loss: 1.0142 - val_acc: 0.8439\n",
      "Epoch 75/100\n",
      "468/468 [==============================] - 103s 220ms/step - loss: 1.0881 - acc: 0.8134 - val_loss: 1.0087 - val_acc: 0.8445\n",
      "Epoch 76/100\n",
      "468/468 [==============================] - 100s 215ms/step - loss: 1.0753 - acc: 0.8157 - val_loss: 0.9993 - val_acc: 0.8416\n",
      "Epoch 77/100\n",
      "468/468 [==============================] - 102s 217ms/step - loss: 1.0620 - acc: 0.8189 - val_loss: 0.9927 - val_acc: 0.8374\n",
      "Epoch 78/100\n",
      "468/468 [==============================] - 104s 223ms/step - loss: 1.0595 - acc: 0.8149 - val_loss: 0.9795 - val_acc: 0.8434\n",
      "Epoch 79/100\n",
      "468/468 [==============================] - 100s 215ms/step - loss: 1.0491 - acc: 0.8170 - val_loss: 0.9700 - val_acc: 0.8451\n",
      "Epoch 80/100\n",
      "468/468 [==============================] - 103s 219ms/step - loss: 1.0399 - acc: 0.8177 - val_loss: 0.9573 - val_acc: 0.8442\n",
      "Epoch 81/100\n",
      "468/468 [==============================] - 98s 210ms/step - loss: 1.0350 - acc: 0.8160 - val_loss: 0.9544 - val_acc: 0.8425\n",
      "Epoch 82/100\n",
      "468/468 [==============================] - 103s 221ms/step - loss: 1.0219 - acc: 0.8183 - val_loss: 0.9503 - val_acc: 0.8430\n",
      "Epoch 83/100\n",
      "468/468 [==============================] - 100s 215ms/step - loss: 1.0169 - acc: 0.8202 - val_loss: 0.9347 - val_acc: 0.8485\n",
      "Epoch 84/100\n",
      "468/468 [==============================] - 103s 220ms/step - loss: 1.0112 - acc: 0.8197 - val_loss: 0.9290 - val_acc: 0.8495\n",
      "Epoch 85/100\n",
      "468/468 [==============================] - 104s 222ms/step - loss: 0.9991 - acc: 0.8194 - val_loss: 0.9191 - val_acc: 0.8497\n",
      "Epoch 86/100\n",
      "468/468 [==============================] - 101s 215ms/step - loss: 0.9890 - acc: 0.8204 - val_loss: 0.9175 - val_acc: 0.8459\n",
      "Epoch 87/100\n",
      "468/468 [==============================] - 102s 218ms/step - loss: 0.9845 - acc: 0.8207 - val_loss: 0.9034 - val_acc: 0.8504\n",
      "Epoch 88/100\n",
      "468/468 [==============================] - 100s 214ms/step - loss: 0.9766 - acc: 0.8241 - val_loss: 0.9113 - val_acc: 0.8465\n",
      "Epoch 89/100\n",
      "468/468 [==============================] - 101s 216ms/step - loss: 0.9673 - acc: 0.8222 - val_loss: 0.8905 - val_acc: 0.8485\n",
      "Epoch 90/100\n",
      "468/468 [==============================] - 107s 228ms/step - loss: 0.9607 - acc: 0.8238 - val_loss: 0.8892 - val_acc: 0.8464\n",
      "Epoch 91/100\n",
      "468/468 [==============================] - 99s 212ms/step - loss: 0.9515 - acc: 0.8243 - val_loss: 0.8768 - val_acc: 0.8502\n",
      "Epoch 92/100\n",
      "468/468 [==============================] - 100s 213ms/step - loss: 0.9453 - acc: 0.8251 - val_loss: 0.8655 - val_acc: 0.8548\n",
      "Epoch 93/100\n",
      "468/468 [==============================] - 102s 217ms/step - loss: 0.9375 - acc: 0.8237 - val_loss: 0.8631 - val_acc: 0.8516\n",
      "Epoch 94/100\n",
      "468/468 [==============================] - 100s 213ms/step - loss: 0.9304 - acc: 0.8242 - val_loss: 0.8525 - val_acc: 0.8531\n",
      "Epoch 95/100\n",
      "468/468 [==============================] - 102s 217ms/step - loss: 0.9266 - acc: 0.8251 - val_loss: 0.8533 - val_acc: 0.8511\n",
      "Epoch 96/100\n",
      "468/468 [==============================] - 101s 217ms/step - loss: 0.9148 - acc: 0.8250 - val_loss: 0.8393 - val_acc: 0.8531\n",
      "Epoch 97/100\n",
      "468/468 [==============================] - 100s 213ms/step - loss: 0.9116 - acc: 0.8258 - val_loss: 0.8355 - val_acc: 0.8515\n",
      "Epoch 98/100\n",
      "468/468 [==============================] - 99s 212ms/step - loss: 0.9028 - acc: 0.8273 - val_loss: 0.8319 - val_acc: 0.8544\n",
      "Epoch 99/100\n",
      "468/468 [==============================] - 101s 216ms/step - loss: 0.9002 - acc: 0.8252 - val_loss: 0.8199 - val_acc: 0.8553\n",
      "Epoch 100/100\n",
      "468/468 [==============================] - 102s 219ms/step - loss: 0.8864 - acc: 0.8295 - val_loss: 0.8168 - val_acc: 0.8535\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0363164390>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datagen.fit(x_train)\n",
    "modelo_l1_l2.fit_generator(datagen.flow(x_train, y_train,\n",
    "                                     batch_size=batch_size,\n",
    "                                       subset=\"training\"),\n",
    "                        epochs=epocas,\n",
    "                        validation_data=datagen.flow(x_train, y_train,\n",
    "                                     batch_size=batch_size,\n",
    "                                       subset=\"validation\"),\n",
    "                        steps_per_epoch=np.ceil(len(x_train) // batch_size),\n",
    "                        validation_steps=np.ceil(len(x_train) // batch_size),\n",
    "                        workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 5s 465us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7680069026947022, 0.8744]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelo_l1_l2.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede ver que el modelo _no_ está sobreajustado, tiene una excelente generalización al tener una precisión más alta en el conjunto de validación que en el conjunto de entrenamiento. También se puede ver este hecho cuando se evalúa el conjunto de prueba. Es notable que el modelo aún puede mejorar mucho más su precisión, pero el entrenamiento se vuelve lento y muy tardado. En estos casos valdría la pena implementar _Early stopping_ junto con otras técnicas para entrenar durante más tiempo y detener el proceso en caso de no encontrar mejores resultados.\n",
    "\n",
    "En este caso, el **aumento de datos** junto con las demás metodologías de _regularización_ resultó ser una excelente metodología para trabajar con ejemplos adicionales, por lo que al conocer bien el conjunto de datos se puede tomar en cuenta esta técnica en caso de no tener suficientes ejemplos en el conjunto de datos."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
